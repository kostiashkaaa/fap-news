#!/usr/bin/env python3
"""
Google News RSS Collector - —Å–±–æ—Ä –Ω–æ–≤–æ—Å—Ç–µ–π —á–µ—Ä–µ–∑ Google News RSS
–ë–µ—Å–ø–ª–∞—Ç–Ω–æ, –±–µ–∑ API –∫–ª—é—á–∞, —Ä–∞–±–æ—Ç–∞–µ—Ç —Å—Ä–∞–∑—É
"""

import asyncio
import hashlib
import logging
import re
from datetime import datetime
from typing import Dict, List, Optional
from urllib.parse import quote_plus

import feedparser
import httpx

from parser import NewsItem

logger = logging.getLogger(__name__)


class GoogleNewsCollector:
    """–°–±–æ—Ä—â–∏–∫ –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ Google News RSS"""
    
    # –ë–∞–∑–æ–≤—ã–µ URL –¥–ª—è Google News RSS
    BASE_URL = "https://news.google.com/rss"
    SEARCH_URL = "https://news.google.com/rss/search"
    TOPICS_URL = "https://news.google.com/rss/topics"
    
    # –ü—Ä–µ–¥—É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ —Ç–µ–º—ã Google News
    TOPICS = {
        "world": "CAAqJggKIiBDQkFTRWdvSUwyMHZNRGx1YlY4U0FuSjFHZ0pTVlNnQVAB",
        "nation": "CAAqJggKIiBDQkFTRWdvSUwyMHZNRFZxYUdjU0FuSjFHZ0pTVlNnQVAB",
        "business": "CAAqJggKIiBDQkFTRWdvSUwyMHZNRGx6TVdZU0FuSjFHZ0pTVlNnQVAB",
        "technology": "CAAqJggKIiBDQkFTRWdvSUwyMHZNRGRqTVhZU0FuSjFHZ0pTVlNnQVAB",
        "science": "CAAqJggKIiBDQkFTRWdvSUwyMHZNRFp0Y1RjU0FuSjFHZ0pTVlNnQVAB",
        "health": "CAAqIQgKIhtDQkFTRGdvSUwyMHZNR3QwTlRFU0FuSjFLQUFQAQ",
        "sports": "CAAqJggKIiBDQkFTRWdvSUwyMHZNRFp1ZEdvU0FuSjFHZ0pTVlNnQVAB",
    }
    
    def __init__(self, config: Dict):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–±–æ—Ä—â–∏–∫–∞
        
        Args:
            config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å –∫–ª—é—á–∞–º–∏:
                - enabled: bool
                - language: str (ru, en, etc.)
                - country: str (RU, US, etc.)
                - topics: List[str] - —Ç–µ–º—ã –¥–ª—è —Å–±–æ—Ä–∞
                - search_queries: List[str] - –ø–æ–∏—Å–∫–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã
                - max_items_per_source: int
        """
        self.config = config
        self.enabled = config.get("enabled", False)
        self.language = config.get("language", "ru")
        self.country = config.get("country", "RU")
        self.topics = config.get("topics", ["world", "nation"])
        self.search_queries = config.get("search_queries", [
            "–†–æ—Å—Å–∏—è –£–∫—Ä–∞–∏–Ω–∞",
            "Putin",
            "Zelensky",
            "NATO",
            "–≤–æ–π–Ω–∞",
            "—Å–∞–Ω–∫—Ü–∏–∏"
        ])
        self.max_items = config.get("max_items_per_source", 20)
    
    def _make_id(self, url: str) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã–π ID –¥–ª—è –Ω–æ–≤–æ—Å—Ç–∏"""
        return hashlib.sha256(url.encode("utf-8")).hexdigest()[:32]
    
    def _clean_title(self, title: str) -> str:
        """–û—á–∏—â–∞–µ—Ç –∑–∞–≥–æ–ª–æ–≤–æ–∫ –æ—Ç –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤"""
        # –£–±–∏—Ä–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –≤ –∫–æ–Ω—Ü–µ (–æ–±—ã—á–Ω–æ –ø–æ—Å–ª–µ " - ")
        if " - " in title:
            parts = title.rsplit(" - ", 1)
            if len(parts) == 2 and len(parts[1]) < 50:
                title = parts[0]
        return title.strip()
    
    def _extract_source(self, entry) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –∏–∑ –∑–∞–ø–∏—Å–∏"""
        # Google News –¥–æ–±–∞–≤–ª—è–µ—Ç –∏—Å—Ç–æ—á–Ω–∏–∫ –≤ –∫–æ–Ω—Ü–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞
        title = getattr(entry, "title", "")
        if " - " in title:
            parts = title.rsplit(" - ", 1)
            if len(parts) == 2 and len(parts[1]) < 50:
                return parts[1].strip()
        
        # –ò–ª–∏ –∏–∑ –ø–æ–ª—è source
        source = getattr(entry, "source", None)
        if source:
            return getattr(source, "title", "Google News")
        
        return "Google News"
    
    def _build_topic_url(self, topic: str) -> str:
        """–°—Ç—Ä–æ–∏—Ç URL –¥–ª—è —Ç–µ–º—ã"""
        topic_id = self.TOPICS.get(topic.lower())
        if topic_id:
            return f"{self.TOPICS_URL}/{topic_id}?hl={self.language}&gl={self.country}&ceid={self.country}:{self.language}"
        return None
    
    def _build_search_url(self, query: str) -> str:
        """–°—Ç—Ä–æ–∏—Ç URL –¥–ª—è –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞"""
        encoded_query = quote_plus(query)
        return f"{self.SEARCH_URL}?q={encoded_query}&hl={self.language}&gl={self.country}&ceid={self.country}:{self.language}"
    
    def _parse_feed(self, url: str, tag: str) -> List[NewsItem]:
        """–ü–∞—Ä—Å–∏—Ç RSS —Ñ–∏–¥ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –Ω–æ–≤–æ—Å—Ç–µ–π"""
        items = []
        
        try:
            feed = feedparser.parse(url)
            
            for entry in feed.entries[:self.max_items]:
                title = self._clean_title(getattr(entry, "title", ""))
                link = getattr(entry, "link", "")
                
                if not title or not link:
                    continue
                
                # –ü–∞—Ä—Å–∏–º –¥–∞—Ç—É –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
                published_parsed = getattr(entry, "published_parsed", None)
                if published_parsed:
                    published_at = datetime(*published_parsed[:6]).isoformat()
                else:
                    published_at = datetime.utcnow().isoformat()
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ
                summary = getattr(entry, "summary", "")
                # –£–±–∏—Ä–∞–µ–º HTML —Ç–µ–≥–∏ –∏–∑ summary
                summary = re.sub(r'<[^>]+>', '', summary)
                
                source = self._extract_source(entry)
                
                items.append(NewsItem(
                    id=self._make_id(link),
                    title=title,
                    link=link,
                    summary=summary[:500] if summary else "",
                    source=f"Google News ({source})",
                    published_at=published_at,
                    tag=tag
                ))
            
        except Exception as e:
            logger.warning(f"Failed to parse Google News feed: {e}")
        
        return items
    
    def collect_by_topics(self) -> List[NewsItem]:
        """–°–æ–±–∏—Ä–∞–µ—Ç –Ω–æ–≤–æ—Å—Ç–∏ –ø–æ —Ç–µ–º–∞–º"""
        if not self.enabled:
            return []
        
        all_items = []
        
        for topic in self.topics:
            url = self._build_topic_url(topic)
            if url:
                items = self._parse_feed(url, f"#gnews_{topic}")
                all_items.extend(items)
                logger.info(f"üì∞ Google News [{topic}]: collected {len(items)} items")
        
        return all_items
    
    def collect_by_search(self) -> List[NewsItem]:
        """–°–æ–±–∏—Ä–∞–µ—Ç –Ω–æ–≤–æ—Å—Ç–∏ –ø–æ –ø–æ–∏—Å–∫–æ–≤—ã–º –∑–∞–ø—Ä–æ—Å–∞–º"""
        if not self.enabled:
            return []
        
        all_items = []
        
        for query in self.search_queries:
            url = self._build_search_url(query)
            # –°–æ–∑–¥–∞–µ–º –±–µ–∑–æ–ø–∞—Å–Ω—ã–π —Ç–µ–≥ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞
            safe_tag = re.sub(r'[^\w]', '_', query.lower())[:20]
            items = self._parse_feed(url, f"#gnews_{safe_tag}")
            all_items.extend(items)
            logger.info(f"üì∞ Google News [{query}]: collected {len(items)} items")
        
        return all_items
    
    def collect_all(self) -> List[NewsItem]:
        """–°–æ–±–∏—Ä–∞–µ—Ç –≤—Å–µ –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ Google News"""
        if not self.enabled:
            logger.debug("Google News collector is disabled")
            return []
        
        all_items = []
        
        # –°–æ–±–∏—Ä–∞–µ–º –ø–æ —Ç–µ–º–∞–º
        all_items.extend(self.collect_by_topics())
        
        # –°–æ–±–∏—Ä–∞–µ–º –ø–æ –ø–æ–∏—Å–∫–æ–≤—ã–º –∑–∞–ø—Ä–æ—Å–∞–º  
        all_items.extend(self.collect_by_search())
        
        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ ID
        seen_ids = set()
        unique_items = []
        for item in all_items:
            if item.id not in seen_ids:
                seen_ids.add(item.id)
                unique_items.append(item)
        
        logger.info(f"üì∞ Google News total: {len(unique_items)} unique items")
        return unique_items


def create_google_news_config() -> Dict:
    """–°–æ–∑–¥–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –¥–ª—è Google News"""
    return {
        "enabled": True,
        "language": "ru",
        "country": "RU",
        "topics": ["world", "nation", "business"],
        "search_queries": [
            "–†–æ—Å—Å–∏—è –£–∫—Ä–∞–∏–Ω–∞",
            "–ü—É—Ç–∏–Ω",
            "–ó–µ–ª–µ–Ω—Å–∫–∏–π",
            "NATO –ù–ê–¢–û",
            "—Å–∞–Ω–∫—Ü–∏–∏ sanctions",
            "–≤–æ–π–Ω–∞ war",
            "–°–®–ê –ï–≤—Ä–æ–ø–∞",
            "–ö–∏—Ç–∞–π China"
        ],
        "max_items_per_source": 15
    }


# –¢–µ—Å—Ç
if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    
    config = create_google_news_config()
    collector = GoogleNewsCollector(config)
    
    items = collector.collect_all()
    
    print(f"\nCollected {len(items)} items:")
    for item in items[:10]:
        print(f"- [{item.tag}] {item.title[:70]}...")
        print(f"  Source: {item.source}")
        print()
