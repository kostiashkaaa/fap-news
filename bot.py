#!/usr/bin/env python3
"""
FAP News - Main Bot
Collects news from multiple sources, applies AI analysis, and posts to Telegram
"""

from __future__ import annotations

import asyncio
import json
import logging
import os
import random
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Tuple, Any
from collections import deque

from apscheduler.schedulers.asyncio import AsyncIOScheduler
from dotenv import load_dotenv

from db import init_db, is_published, mark_published
from parser import NewsItem, collect_news, load_config
from poster import post_news_item
from smart_deduplicator import SmartDeduplicator
from alternative_sources import AlternativeNewsCollector


logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s %(levelname)s %(name)s: %(message)s",
)

# –ì–ª–æ–±–∞–ª—å–Ω–∞—è –æ—á–µ—Ä–µ–¥—å –ø–æ—Å—Ç–æ–≤
post_queue: deque = deque()
logger = logging.getLogger("fap-news")


ROOT = Path(__file__).parent
CONFIG_PATH = ROOT / "config.json"


async def process_urgent_news(items: List[NewsItem], config: Dict) -> None:
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Å—Ä–æ—á–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏ –∏ –ø—É–±–ª–∏–∫—É–µ—Ç –∏—Ö –Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ"""
    if not items:
        return
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º AI summarizer –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å—Ä–æ—á–Ω–æ—Å—Ç–∏
    ai_config = config.get("ai_summarization", {})
    if not ai_config.get("enabled", False):
        return
    
    from ai_summarizer import AISummarizer
    summarizer = AISummarizer(ai_config)
    
    if not summarizer.is_enabled():
        return
    
    telegram_cfg = config.get("telegram", {})
    token = telegram_cfg.get("token") or os.getenv("TELEGRAM_BOT_TOKEN")
    channel_id = telegram_cfg.get("channel_id")
    
    if not token or not channel_id:
        logger.warning("Telegram credentials not available for urgent news")
        return
    
    # –ü–æ–ª—É—á–∞–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫—É –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –≤–æ–∑—Ä–∞—Å—Ç–∞ –Ω–æ–≤–æ—Å—Ç–µ–π
    filters_cfg = config.get("filters", {})
    max_age_minutes = filters_cfg.get("max_age_minutes", 120)  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é 2 —á–∞—Å–∞
    
    urgent_items = []
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∂–¥—É—é –Ω–æ–≤–æ—Å—Ç—å –Ω–∞ —Å—Ä–æ—á–Ω–æ—Å—Ç—å –∏ —Å–≤–µ–∂–µ—Å—Ç—å (–æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ API)
    rate_limit_cfg = ai_config.get("rate_limit", {})
    max_checks = rate_limit_cfg.get("max_urgency_checks", 10)
    items_to_check = [item for item in items if not is_published(item.id, item.source)][:max_checks]
    
    logger.info(f"üîç Checking {len(items_to_check)} items for urgency (limited to {max_checks} to save API calls)")
    
    for item in items_to_check:
        try:
            # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º —Å–≤–µ–∂–µ—Å—Ç—å
            is_fresh = await summarizer.check_news_freshness(
                title=item.title,
                content=item.summary or "",
                max_age_minutes=max_age_minutes
            )
            
            if not is_fresh:
                logger.info(f"‚è∞ OLD NEWS SKIPPED: '{item.title[:50]}...' from {item.source} (older than {max_age_minutes} minutes)")
                continue
            
            # –ó–∞—Ç–µ–º –ø—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ä–æ—á–Ω–æ—Å—Ç—å
            is_urgent = await summarizer.check_urgency(
                title=item.title,
                content=item.summary or ""
            )
            
            if is_urgent:
                urgent_items.append(item)
                logger.info(f"üö® URGENT NEWS DETECTED: '{item.title[:50]}...' from {item.source}")
                
        except Exception as e:
            logger.warning(f"Failed to check urgency/freshness for '{item.title[:50]}...': {e}")
    
    # –ü—É–±–ª–∏–∫—É–µ–º –≤—Å–µ —Å—Ä–æ—á–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏ –Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ
    if urgent_items:
        logger.info(f"üö® Publishing {len(urgent_items)} urgent news items immediately")
        
        for item in urgent_items:
            try:
                await post_news_item(
                    item, config, 
                    bot_token=token, channel_id=channel_id, 
                    is_urgent=True
                )
                mark_published(item.id, item.link, item.source, item.published_at)
                logger.info(f"‚ö° URGENT POSTED: '{item.title[:50]}...' from {item.source}")
                
                # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É —Å—Ä–æ—á–Ω—ã–º–∏ –ø–æ—Å—Ç–∞–º–∏
                await asyncio.sleep(2)
                
            except Exception as e:
                logger.error(f"‚ùå Failed to post urgent news '{item.title[:50]}...': {e}")


async def process_post_queue() -> None:
    """–ü—É–±–ª–∏–∫—É–µ—Ç –æ–¥–∏–Ω –ø–æ—Å—Ç –∏–∑ –æ—á–µ—Ä–µ–¥–∏ –∫–∞–∂–¥—ã–µ 2-4 –º–∏–Ω—É—Ç—ã"""
    global post_queue
    
    if not post_queue:
        return
    
    # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π –ø–æ—Å—Ç –∏–∑ –æ—á–µ—Ä–µ–¥–∏
    item, config, token, channel_id = post_queue.popleft()
    
    try:
        await post_news_item(item, config, bot_token=token, channel_id=channel_id)
        mark_published(item.id, item.link, item.source, item.published_at)
        logger.info(f"‚úÖ Posted: '{item.title[:50]}...' from {item.source}")
    except Exception as e:
        logger.error(f"‚ùå Failed to post '{item.title[:50]}...': {e}")
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–æ—Å—Ç –≤ –∫–æ–Ω–µ—Ü –æ—á–µ—Ä–µ–¥–∏ –¥–ª—è –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –ø–æ–ø—ã—Ç–∫–∏
        post_queue.append((item, config, token, channel_id))


async def post_single_item(item, translation_cfg, token, channel_id):
    """Post a single news item with error handling (deprecated - use queue)"""
    try:
        await post_news_item(item, translation_cfg, bot_token=token, channel_id=channel_id)
        mark_published(item.id, item.link, item.source, item.published_at)
        logger.info(f"Posted: {item.title[:50]}...")
    except Exception as e:
        logger.exception("Failed to post item '%s': %s", item.title, e)


async def process_once(config: Dict) -> None:
    init_db()
    logger.info("Collecting news from %d sources", len(config.get("sources", [])))

    # –°–æ–±–∏—Ä–∞–µ–º –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ RSS –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
    items: List[NewsItem] = collect_news(config)
    logger.info("Collected %d items from RSS sources", len(items))
    
    # –°–æ–±–∏—Ä–∞–µ–º –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
    alt_config = config.get("alternative_sources", {})
    if any(source.get("enabled", False) for source in alt_config.values() if isinstance(source, dict)):
        try:
            async with AlternativeNewsCollector(alt_config) as alt_collector:
                alt_items = await alt_collector.collect_all_alternative_sources()
                items.extend(alt_items)
                logger.info("Collected %d items from alternative sources", len(alt_items))
        except Exception as e:
            logger.error(f"Failed to collect from alternative sources: {e}")
    
    logger.info("Total collected %d items before dedup/filter", len(items))
    
    # –ü—Ä–∏–º–µ–Ω—è–µ–º —É–º–Ω—É—é –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—é
    dedup_config = config.get("deduplication", {
        "enabled": True,
        "similarity_threshold": 0.7,
        "title_weight": 0.6,
        "content_weight": 0.4
    })
    
    if dedup_config.get("enabled", True):
        deduplicator = SmartDeduplicator(dedup_config)
        unique_items, duplicate_items = deduplicator.filter_duplicates(items)
        items = unique_items
        logger.info("After deduplication: %d unique items, %d duplicates filtered", len(items), len(duplicate_items))

    telegram_cfg = config.get("telegram", {})

    token = telegram_cfg.get("token") or os.getenv("TELEGRAM_BOT_TOKEN")
    channel_id = telegram_cfg.get("channel_id")

    # –°–Ω–∞—á–∞–ª–∞ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å—Ä–æ—á–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏
    await process_urgent_news(items, config)

    # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ –ø–æ—Å—Ç—ã –≤ –æ—á–µ—Ä–µ–¥—å (–º–∞–∫—Å–∏–º—É–º 3 –∑–∞ —Ä–∞–∑, –∏–∑ —Ä–∞–∑–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤)
    # –°–Ω–∞—á–∞–ª–∞ —Ñ–∏–ª—å—Ç—Ä—É–µ–º –ø–æ —Å–≤–µ–∂–µ—Å—Ç–∏
    fresh_items = []
    filters_cfg = config.get("filters", {})
    max_age_minutes = filters_cfg.get("max_age_minutes", 120)  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é 2 —á–∞—Å–∞
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º AI summarizer –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å–≤–µ–∂–µ—Å—Ç–∏
    ai_config = config.get("ai_summarization", {})
    if ai_config.get("enabled", False):
        from ai_summarizer import AISummarizer
        summarizer = AISummarizer(ai_config)
        
        if summarizer.is_enabled():
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ–≤–µ—Ä–æ–∫ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ API
            rate_limit_cfg = ai_config.get("rate_limit", {})
            max_freshness_checks = rate_limit_cfg.get("max_freshness_checks", 15)
            items_to_check = [item for item in items if not is_published(item.id, item.source)][:max_freshness_checks]
            
            logger.info(f"üîç Checking freshness of {len(items_to_check)} items (max age: {max_age_minutes} minutes, limited to {max_freshness_checks} to save API calls)")
            
            for item in items_to_check:
                try:
                    is_fresh = await summarizer.check_news_freshness(
                        title=item.title,
                        content=item.summary or "",
                        max_age_minutes=max_age_minutes
                    )
                    
                    if is_fresh:
                        fresh_items.append(item)
                    else:
                        logger.info(f"‚è∞ OLD NEWS FILTERED: '{item.title[:50]}...' from {item.source}")
                        
                except Exception as e:
                    logger.warning(f"Failed to check freshness for '{item.title[:50]}...': {e}")
                    # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å, –¥–æ–±–∞–≤–ª—è–µ–º –≤ —Å–ø–∏—Å–æ–∫
                    fresh_items.append(item)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏ –±–µ–∑ –ø—Ä–æ–≤–µ—Ä–∫–∏ (–µ—Å–ª–∏ –∏—Ö –±–æ–ª—å—à–µ —á–µ–º –ª–∏–º–∏—Ç)
            remaining_items = [item for item in items if not is_published(item.id, item.source)][max_freshness_checks:]
            fresh_items.extend(remaining_items)
            if remaining_items:
                logger.info(f"üì∞ Added {len(remaining_items)} items without freshness check (API limit reached)")
        else:
            # –ï—Å–ª–∏ AI –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –±–µ—Ä–µ–º –≤—Å–µ –Ω–æ–≤–æ—Å—Ç–∏
            fresh_items = [item for item in items if not is_published(item.id, item.source)]
    else:
        # –ï—Å–ª–∏ AI –æ—Ç–∫–ª—é—á–µ–Ω, –±–µ—Ä–µ–º –≤—Å–µ –Ω–æ–≤–æ—Å—Ç–∏
        fresh_items = [item for item in items if not is_published(item.id, item.source)]
    
    logger.info(f"üì∞ Fresh items after filtering: {len(fresh_items)}")
    
    new_items = fresh_items
    
    if new_items:
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º –¥–ª—è –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
        from collections import defaultdict
        items_by_source = defaultdict(list)
        for item in new_items:
            items_by_source[item.source].append(item)
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—ã –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        priority_sources = {
            # –í—ã—Å–æ–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç - –µ–≤—Ä–æ–ø–µ–π—Å–∫–∏–µ –∏ –∞–º–µ—Ä–∏–∫–∞–Ω—Å–∫–∏–µ –°–ú–ò
            'high': [
                'Fox News', 'New York Times World', 'Financial Times World',
                'Washington Post World', 'The Guardian World', 'BBC News Russian',
                'Euronews', 'Deutsche Welle', 'France 24', 'Al Jazeera'
            ],
            # –°—Ä–µ–¥–Ω–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç - –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ –∏ –∞–∑–∏–∞—Ç—Å–∫–∏–µ
            'medium': [
                'South China Morning Post', 'Japan Times', 'Reuters - World News'
            ],
            # –ù–∏–∑–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç - —Ä–æ—Å—Å–∏–π—Å–∫–∏–µ –°–ú–ò
            'low': [
                'RT Russian', 'TASS', 'RIA Novosti', 'Lenta.ru'
            ]
        }
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É
        prioritized_sources = []
        
        # –°–Ω–∞—á–∞–ª–∞ –≤—ã—Å–æ–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
        for source in priority_sources['high']:
            if source in items_by_source and items_by_source[source]:
                prioritized_sources.append(source)
        
        # –ü–æ—Ç–æ–º —Å—Ä–µ–¥–Ω–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
        for source in priority_sources['medium']:
            if source in items_by_source and items_by_source[source]:
                prioritized_sources.append(source)
        
        # –ü–æ—Ç–æ–º –Ω–∏–∑–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
        for source in priority_sources['low']:
            if source in items_by_source and items_by_source[source]:
                prioritized_sources.append(source)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ (–µ—Å–ª–∏ –µ—Å—Ç—å –Ω–æ–≤—ã–µ)
        for source in items_by_source:
            if source not in prioritized_sources:
                prioritized_sources.append(source)
        
        # –ë–µ—Ä–µ–º –ø–æ 1 –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ (–º–∞–∫—Å–∏–º—É–º 3 –∏—Å—Ç–æ—á–Ω–∏–∫–∞)
        selected_items = []
        sources_used = 0
        max_sources = 3
        
        for source in prioritized_sources:
            if sources_used >= max_sources:
                break
            if source in items_by_source and items_by_source[source]:
                selected_items.append(items_by_source[source][0])  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—É—é –Ω–æ–≤–æ—Å—Ç—å –∏–∑ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
                sources_used += 1
        
        # –õ–æ–≥–∏—Ä—É–µ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—ã
        selected_sources = [item.source for item in selected_items]
        logger.info(f"Priority sources selected: {selected_sources}")
        logger.info(f"Adding {len(selected_items)} new items to posting queue from {sources_used} sources")
        
        # –î–æ–±–∞–≤–ª—è–µ–º –≤ –≥–ª–æ–±–∞–ª—å–Ω—É—é –æ—á–µ—Ä–µ–¥—å –ø–æ—Å—Ç–æ–≤
        global post_queue
        for item in selected_items:
            post_queue.append((item, config, token, channel_id))
            logger.info(f"Added to queue: '{item.title[:50]}...' from {item.source}")
    else:
        logger.info("No new items to post")


async def scheduler_main() -> None:
    load_dotenv()
    if not CONFIG_PATH.exists():
        raise RuntimeError(f"Config file not found at {CONFIG_PATH}")
    config = load_config(str(CONFIG_PATH))

    interval = int(config.get("scheduler", {}).get("interval_minutes", 10))

    # Run once at start
    await process_once(config)

    scheduler = AsyncIOScheduler()
    # –°–æ–±–∏—Ä–∞–µ–º –Ω–æ–≤–æ—Å—Ç–∏ –∫–∞–∂–¥—ã–µ 10 –º–∏–Ω—É—Ç
    scheduler.add_job(process_once, "interval", minutes=interval, args=[config], id="collect-and-post")
    
    # –ü—É–±–ª–∏–∫—É–µ–º –∏–∑ –æ—á–µ—Ä–µ–¥–∏ –∫–∞–∂–¥—ã–µ 6 –º–∏–Ω—É—Ç (—Å—Ä–µ–¥–Ω–∏–π –∏–Ω—Ç–µ—Ä–≤–∞–ª –º–µ–∂–¥—É 5-7)
    posting_cfg = config.get("posting", {})
    min_delay = posting_cfg.get("min_delay_minutes", 5)
    max_delay = posting_cfg.get("max_delay_minutes", 7)
    avg_delay = (min_delay + max_delay) // 2
    
    # –ü–µ—Ä–≤–∞—è –ø—É–±–ª–∏–∫–∞—Ü–∏—è —á–µ—Ä–µ–∑ —Å–ª—É—á–∞–π–Ω–æ–µ –≤—Ä–µ–º—è
    first_post_delay = random.randint(min_delay, max_delay)
    scheduler.add_job(process_post_queue, "interval", minutes=avg_delay, id="post-from-queue")
    
    scheduler.start()

    logger.info("Scheduler started:")
    logger.info("  - Collecting news every %d minutes", interval)
    logger.info("  - Posting from queue every %d-%d minutes", min_delay, max_delay)

    # Keep the event loop alive
    try:
        while True:
            await asyncio.sleep(3600)
    except (KeyboardInterrupt, SystemExit):
        logger.info("Shutting down...")
        scheduler.shutdown(wait=False)


if __name__ == "__main__":
    asyncio.run(scheduler_main())
