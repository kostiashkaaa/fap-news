#!/usr/bin/env python3
"""
Smart Deduplicator - —É–º–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ —Å–º—ã—Å–ª—É
"""

import hashlib
import logging
import re
from dataclasses import dataclass
from typing import List, Dict, Set, Optional, Tuple
from difflib import SequenceMatcher

from parser import NewsItem

logger = logging.getLogger(__name__)

@dataclass
class SimilarityResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –Ω–æ–≤–æ—Å—Ç–µ–π"""
    similarity_score: float  # 0-1
    is_duplicate: bool
    reason: str

class SmartDeduplicator:
    """–£–º–Ω—ã–π –¥–µ–¥—É–ø–ª–∏–∫–∞—Ç–æ—Ä –Ω–æ–≤–æ—Å—Ç–µ–π"""
    
    def __init__(self, config: Dict):
        self.config = config
        self.similarity_threshold = config.get("similarity_threshold", 0.7)
        self.title_weight = config.get("title_weight", 0.6)
        self.content_weight = config.get("content_weight", 0.4)
        self.min_content_length = config.get("min_content_length", 20)
        
        # –ö—ç—à –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        self._processed_hashes: Set[str] = set()
        self._content_hashes: Dict[str, NewsItem] = {}
    
    def _normalize_text(self, text: str) -> str:
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è"""
        if not text:
            return ""
        
        # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
        text = text.lower()
        
        # –£–¥–∞–ª—è–µ–º HTML —Ç–µ–≥–∏
        text = re.sub(r'<[^>]+>', '', text)
        
        # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫
        text = re.sub(r'\s+', ' ', text).strip()
        
        # –£–¥–∞–ª—è–µ–º –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è –¥–ª—è –ª—É—á—à–µ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        text = re.sub(r'[^\w\s]', ' ', text)
        
        # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    def _extract_key_phrases(self, text: str) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Ñ—Ä–∞–∑—ã –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        normalized = self._normalize_text(text)
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Å–ª–æ–≤–∞
        words = normalized.split()
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
        stop_words = {
            '–∏', '–≤', '–Ω–∞', '—Å', '–ø–æ', '–¥–ª—è', '–æ—Ç', '–¥–æ', '–∏–∑', '–∫', '—É', '–æ', '–æ–±', '–∑–∞', '–ø—Ä–∏',
            'the', 'and', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'from', 'up', 'about',
            'a', 'an', 'is', 'are', 'was', 'were', 'be', 'been', 'have', 'has', 'had', 'do', 'does', 'did',
            'will', 'would', 'could', 'should', 'may', 'might', 'can', 'this', 'that', 'these', 'those',
            '–æ–±—ä—è–≤–∏–ª', '–æ–±—ä—è–≤–∏–ª–∞', '–æ–±—ä—è–≤–∏–ª–∏', '–∑–∞—è–≤–∏–ª', '–∑–∞—è–≤–∏–ª–∞', '–∑–∞—è–≤–∏–ª–∏', '—Å–æ–æ–±—â–∏–ª', '—Å–æ–æ–±—â–∏–ª–∞', '—Å–æ–æ–±—â–∏–ª–∏'
        }
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–Ω–∞—á–∏–º—ã–µ —Å–ª–æ–≤–∞ (–¥–ª–∏–Ω–Ω–µ–µ 2 —Å–∏–º–≤–æ–ª–æ–≤, –Ω–µ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞)
        key_words = [word for word in words if len(word) > 2 and word not in stop_words]
        
        # –°–æ–∑–¥–∞–µ–º –±–∏–≥—Ä–∞–º–º—ã –∏ —Ç—Ä–∏–≥—Ä–∞–º–º—ã –¥–ª—è –ª—É—á—à–µ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        bigrams = []
        trigrams = []
        
        for i in range(len(key_words) - 1):
            bigrams.append(f"{key_words[i]} {key_words[i+1]}")
            
        for i in range(len(key_words) - 2):
            trigrams.append(f"{key_words[i]} {key_words[i+1]} {key_words[i+2]}")
        
        return key_words + bigrams + trigrams
    
    def _calculate_similarity(self, text1: str, text2: str) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Å—Ö–æ–∂–µ—Å—Ç—å –º–µ–∂–¥—É –¥–≤—É–º—è —Ç–µ–∫—Å—Ç–∞–º–∏"""
        if not text1 or not text2:
            return 0.0
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Ç–µ–∫—Å—Ç—ã
        norm1 = self._normalize_text(text1)
        norm2 = self._normalize_text(text2)
        
        if not norm1 or not norm2:
            return 0.0
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º SequenceMatcher –¥–ª—è –±–∞–∑–æ–≤–æ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        basic_similarity = SequenceMatcher(None, norm1, norm2).ratio()
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Ñ—Ä–∞–∑—ã
        phrases1 = set(self._extract_key_phrases(text1))
        phrases2 = set(self._extract_key_phrases(text2))
        
        if not phrases1 or not phrases2:
            return basic_similarity
        
        # –í—ã—á–∏—Å–ª—è–µ–º –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Ñ—Ä–∞–∑
        intersection = phrases1.intersection(phrases2)
        union = phrases1.union(phrases2)
        
        phrase_similarity = len(intersection) / len(union) if union else 0.0
        
        # –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        combined_similarity = (basic_similarity * 0.3) + (phrase_similarity * 0.7)
        
        return min(combined_similarity, 1.0)
    
    def _generate_content_hash(self, item: NewsItem) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ö—ç—à –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞"""
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è —Ö—ç—à–∞
        normalized_title = self._normalize_text(item.title)
        normalized_content = self._normalize_text(item.summary or "")
        
        # –°–æ–∑–¥–∞–µ–º —Ö—ç—à –∏–∑ –∫–ª—é—á–µ–≤—ã—Ö —Ñ—Ä–∞–∑
        key_phrases = self._extract_key_phrases(f"{normalized_title} {normalized_content}")
        key_phrases_str = " ".join(sorted(key_phrases))
        
        return hashlib.md5(key_phrases_str.encode('utf-8')).hexdigest()[:12]
    
    def find_duplicates(self, items: List[NewsItem]) -> List[SimilarityResult]:
        """
        –ù–∞—Ö–æ–¥–∏—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã –≤ —Å–ø–∏—Å–∫–µ –Ω–æ–≤–æ—Å—Ç–µ–π
        
        Args:
            items: –°–ø–∏—Å–æ–∫ –Ω–æ–≤–æ—Å—Ç–µ–π –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        """
        results = []
        
        for i, item in enumerate(items):
            if not item.title or len(item.title.strip()) < 5:
                results.append(SimilarityResult(0.0, False, "–°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π –∑–∞–≥–æ–ª–æ–≤–æ–∫"))
                continue
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –±—ã—Å—Ç—Ä—ã–π —Ö—ç—à-–∫—ç—à
            content_hash = self._generate_content_hash(item)
            if content_hash in self._processed_hashes:
                results.append(SimilarityResult(1.0, True, "–¢–æ—á–Ω—ã–π –¥—É–±–ª–∏–∫–∞—Ç –ø–æ —Ö—ç—à—É"))
                continue
            
            # –ò—â–µ–º –ø–æ—Ö–æ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏
            max_similarity = 0.0
            best_match = None
            
            for j, other_item in enumerate(items):
                if i == j or not other_item.title:
                    continue
                
                # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏
                title_similarity = self._calculate_similarity(item.title, other_item.title)
                
                # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ (–µ—Å–ª–∏ –µ—Å—Ç—å)
                content_similarity = 0.0
                if item.summary and other_item.summary:
                    content_similarity = self._calculate_similarity(item.summary, other_item.summary)
                
                # –í—ã—á–∏—Å–ª—è–µ–º –æ–±—â—É—é —Å—Ö–æ–∂–µ—Å—Ç—å
                total_similarity = (title_similarity * self.title_weight + 
                                  content_similarity * self.content_weight)
                
                if total_similarity > max_similarity:
                    max_similarity = total_similarity
                    best_match = other_item
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –¥—É–±–ª–∏–∫–∞—Ç–æ–º
            is_duplicate = max_similarity >= self.similarity_threshold
            
            if is_duplicate:
                reason = f"–ü–æ—Ö–æ–∂ –Ω–∞ '{best_match.title[:50]}...' (—Å—Ö–æ–∂–µ—Å—Ç—å: {max_similarity:.2f})"
            else:
                reason = f"–£–Ω–∏–∫–∞–ª—å–Ω–∞—è –Ω–æ–≤–æ—Å—Ç—å (–º–∞–∫—Å. —Å—Ö–æ–∂–µ—Å—Ç—å: {max_similarity:.2f})"
            
            results.append(SimilarityResult(max_similarity, is_duplicate, reason))
            
            # –î–æ–±–∞–≤–ª—è–µ–º –≤ –∫—ç—à —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –Ω–µ –¥—É–±–ª–∏–∫–∞—Ç
            if not is_duplicate:
                self._processed_hashes.add(content_hash)
                self._content_hashes[content_hash] = item
        
        return results
    
    def filter_duplicates(self, items: List[NewsItem]) -> Tuple[List[NewsItem], List[NewsItem]]:
        """
        –§–∏–ª—å—Ç—Ä—É–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã –∏–∑ —Å–ø–∏—Å–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π
        
        Args:
            items: –°–ø–∏—Å–æ–∫ –Ω–æ–≤–æ—Å—Ç–µ–π
            
        Returns:
            –ö–æ—Ä—Ç–µ–∂ (—É–Ω–∏–∫–∞–ª—å–Ω—ã–µ_–Ω–æ–≤–æ—Å—Ç–∏, –¥—É–±–ª–∏–∫–∞—Ç—ã)
        """
        if not items:
            return [], []
        
        logger.info(f"üîç Checking {len(items)} items for duplicates...")
        
        results = self.find_duplicates(items)
        
        unique_items = []
        duplicate_items = []
        
        for item, result in zip(items, results):
            if result.is_duplicate:
                duplicate_items.append(item)
                logger.info(f"üö´ DUPLICATE: '{item.title[:50]}...' - {result.reason}")
            else:
                unique_items.append(item)
        
        logger.info(f"‚úÖ Found {len(duplicate_items)} duplicates, {len(unique_items)} unique items")
        
        return unique_items, duplicate_items
    
    def get_stats(self) -> Dict[str, int]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –¥–µ–¥—É–ø–ª–∏–∫–∞—Ç–æ—Ä–∞"""
        return {
            "processed_hashes": len(self._processed_hashes),
            "content_hashes": len(self._content_hashes),
            "similarity_threshold": self.similarity_threshold
        }


def test_deduplicator():
    """–¢–µ—Å—Ç –¥–µ–¥—É–ø–ª–∏–∫–∞—Ç–æ—Ä–∞"""
    from parser import NewsItem
    
    # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –Ω–æ–≤–æ—Å—Ç–∏
    items = [
        NewsItem(
            id="1",
            title="–ü—Ä–µ–∑–∏–¥–µ–Ω—Ç –°–®–ê –æ–±—ä—è–≤–∏–ª –æ –Ω–æ–≤—ã—Ö —Å–∞–Ω–∫—Ü–∏—è—Ö –ø—Ä–æ—Ç–∏–≤ –†–æ—Å—Å–∏–∏",
            summary="–ë–µ–ª—ã–π –¥–æ–º –≤–≤–µ–ª –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è",
            link="https://example.com/1",
            source="CNN",
            published_at="2025-01-16T10:00:00Z",
            tag="#cnn"
        ),
        NewsItem(
            id="2", 
            title="–°–®–ê –≤–≤–æ–¥—è—Ç –Ω–æ–≤—ã–µ —Å–∞–Ω–∫—Ü–∏–∏ –ø—Ä–æ—Ç–∏–≤ –†–§",
            summary="–ê–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ü–∏—è –ë–∞–π–¥–µ–Ω–∞ –æ–±—ä—è–≤–∏–ª–∞ –æ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –º–µ—Ä–∞—Ö",
            link="https://example.com/2",
            source="BBC",
            published_at="2025-01-16T10:05:00Z",
            tag="#bbc"
        ),
        NewsItem(
            id="3",
            title="–ö—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ –≤—ã—Ä–æ—Å –Ω–∞ 2%",
            summary="–ê–º–µ—Ä–∏–∫–∞–Ω—Å–∫–∞—è –≤–∞–ª—é—Ç–∞ —É–∫—Ä–µ–ø–∏–ª–∞—Å—å –Ω–∞ –º–∏—Ä–æ–≤—ã—Ö —Ä—ã–Ω–∫–∞—Ö",
            link="https://example.com/3",
            source="Reuters",
            published_at="2025-01-16T10:10:00Z",
            tag="#reuters"
        )
    ]
    
    config = {
        "similarity_threshold": 0.6,  # –†–∞–∑—É–º–Ω—ã–π –ø–æ—Ä–æ–≥ –¥–ª—è –ø—Ä–æ–¥–∞–∫—à–µ–Ω–∞
        "title_weight": 0.7,
        "content_weight": 0.3
    }
    
    deduplicator = SmartDeduplicator(config)
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –Ω–∞–ø—Ä—è–º—É—é
    print("=== –¢–µ—Å—Ç —Å—Ä–∞–≤–Ω–µ–Ω–∏—è ===")
    item1 = items[0]
    item2 = items[1]
    
    similarity = deduplicator._calculate_similarity(item1.title, item2.title)
    print(f"–°—Ö–æ–∂–µ—Å—Ç—å '{item1.title}' –∏ '{item2.title}': {similarity:.3f}")
    
    phrases1 = deduplicator._extract_key_phrases(item1.title)
    phrases2 = deduplicator._extract_key_phrases(item2.title)
    print(f"–ö–ª—é—á–µ–≤—ã–µ —Ñ—Ä–∞–∑—ã 1: {phrases1[:10]}")
    print(f"–ö–ª—é—á–µ–≤—ã–µ —Ñ—Ä–∞–∑—ã 2: {phrases2[:10]}")
    
    unique, duplicates = deduplicator.filter_duplicates(items)
    
    print(f"\n–£–Ω–∏–∫–∞–ª—å–Ω—ã–µ: {len(unique)}")
    print(f"–î—É–±–ª–∏–∫–∞—Ç—ã: {len(duplicates)}")
    
    for item in unique:
        print(f"‚úÖ {item.title}")
    
    for item in duplicates:
        print(f"üö´ {item.title}")


if __name__ == "__main__":
    test_deduplicator()
